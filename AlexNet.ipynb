{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import...\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "mnist = input_data.read_data_sets('/tmp/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 300000\n",
    "batch_size = 64\n",
    "display_step = 100\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "dropout = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv2d\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1,1,1,1], padding='SAME'),b), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_pool\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1,k,k,1], strides=[1,k,k,1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local response normalization\n",
    "def norm(name, l_input, lsize=4):    \n",
    "\n",
    "    # alpha: scale factor, beta: exponential value\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001/9.0, beta=0.75, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "weights = {'wc1': tf.Variable(tf.random_normal([3,3,1,64])),\n",
    "          'wc2': tf.Variable(tf.random_normal([3,3,64,128])),\n",
    "          'wc3': tf.Variable(tf.random_normal([3,3,128,256])),\n",
    "          'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])), # batch_size(64)*1024\n",
    "          'wd2': tf.Variable(tf.random_normal([1024, 1024])), # batch_size*1024 matmul 1024x1024\n",
    "          'out': tf.Variable(tf.random_normal([1024,10]))}  # batch_size(64)x1024 = 내적 = 1024x10\n",
    "\n",
    "biases = {'bc1': tf.Variable(tf.random_normal([64])),\n",
    "         'bc2': tf.Variable(tf.random_normal([128])),\n",
    "         'bc3': tf.Variable(tf.random_normal([256])),\n",
    "         'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "         'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "         'out': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alexnet\n",
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "    \n",
    "    # Conv 1 layer\n",
    "    # 1 = black & number of output\n",
    "    _X = tf.reshape(_X, shape=[-1,28,28,1])\n",
    "    \n",
    "    # weights = # of filters = # of outputs\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    \n",
    "    # pooling 2x2 (0.25 size)\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    \n",
    "    # norm: data/size of vector -> vector=1 fix, find angle\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    \n",
    "    # dropout: reduce overfitting\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "    ###############################################################\n",
    "    # Conv 2 layer\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    pool2 = max_pool('pool2',conv2, k=2)\n",
    "    norm2 = norm('norm2',pool2, lsize=4)\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "    ###############################################################\n",
    "    # Conv 3 layer\n",
    "    conv3 = conv2d('conv3',norm2, _weights['wc3'], _biases['bc3'])\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "    ###############################################################\n",
    "    # FFNN\n",
    "    # Step1. flattening\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Step2. Dense (matmul -> bias)\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2')\n",
    "    \n",
    "    # Step3. Output\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 20:01:29.280263 37356 deprecation.py:506] From <ipython-input-31-2e15833a4341>:18: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "pred = alex_net(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean(prediction - label)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train = Optimizer (learning rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter6400, Minibatch Loss=68513.718750, Training Accuracy=0.39062\n",
      "Iter12800, Minibatch Loss=23020.757812, Training Accuracy=0.71875\n",
      "Iter19200, Minibatch Loss=22225.437500, Training Accuracy=0.70312\n",
      "Iter25600, Minibatch Loss=13790.101562, Training Accuracy=0.82812\n",
      "Iter32000, Minibatch Loss=7043.297852, Training Accuracy=0.82812\n",
      "Iter38400, Minibatch Loss=13035.176758, Training Accuracy=0.76562\n",
      "Iter44800, Minibatch Loss=8624.713867, Training Accuracy=0.90625\n",
      "Iter51200, Minibatch Loss=11614.361328, Training Accuracy=0.78125\n"
     ]
    }
   ],
   "source": [
    "# Session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    \n",
    "    while step*batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "        \n",
    "        if step%display_step == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print('Iter' + str(step*batch_size) + ', Minibatch Loss=' + '{:.6f}'.format(loss) +', Training Accuracy=' + '{:.5f}'.format(acc))\n",
    "        \n",
    "        step += 1\n",
    "    print('Optimization Finished!')\n",
    "    print('Testing Accuracy:', sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                                            y: mnist.test.labels[:256],\n",
    "                                                            keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
