{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-direction RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 12:20:46.382730  6948 deprecation.py:323] From <ipython-input-3-a4a535036686>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0928 12:20:46.385722  6948 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0928 12:20:46.386719  6948 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0928 12:20:46.554193  6948 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0928 12:20:46.554193  6948 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:20:46.594017  6948 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('/tmp/data/', one_hot=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "n_inputs = 28\n",
    "\n",
    "n_steps = 28\n",
    "\n",
    "n_hidden = 128\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, n_steps, n_inputs])\n",
    "y = tf.placeholder('float',[None, n_classes])\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes])) # 이중RNN이라\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수의  reuturn 없는 이유? 데이터 타입이 없기 때문\n",
    "def BiRNN(x, weights, biases):\n",
    "    # 마지막 것 사용을 위해\n",
    "    x = tf.transpose(x, [1,0,2])\n",
    "    \n",
    "    # 1차원 생성: flatten 하는 것\n",
    "    x = tf.reshape(x, [-1,n_inputs])\n",
    "    # 28\n",
    "    x = tf.split(axis=0, num_or_size_splits = n_steps, value=x)\n",
    "    \n",
    "    # lstm gate종류: forget-gate, input-gate, output-gate\n",
    "    # weights가 있음 반드시  biases 있다\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    \n",
    "    try: # _ return 형식값 맞춰주기 위해 _ 씀 (fw_state, bw_state)\n",
    "        outputs,_,_ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "        \n",
    "    except Exception:\n",
    "        # 들어간 이미지 개수만큼  outputs\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    # output 결과 중 마지막 하나 사용\n",
    "    # 앞뒤로 생성되는 값 256개 출력 * (256x10) => 10개 특성으로 빠짐\n",
    "    return tf.matmul(outputs[-1], weights['out'] + biases['out']) #weight 128x2 = 256 x 10 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:37:54.962653  6948 deprecation.py:323] From <ipython-input-11-9d4b4da6dc9b>:17: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
      "W0928 12:37:54.963621  6948 deprecation.py:323] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:1610: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "W0928 12:37:54.979577  6948 deprecation.py:506] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0928 12:37:54.991603  6948 deprecation.py:506] From C:\\Users\\KITCOOP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "pred = BiRNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 12:39:23.584550  6948 deprecation.py:323] From <ipython-input-13-7db698dcc7d3>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter1280\n",
      "Minibatch Loss=1.892155, Training Accuracy=0.37500\n",
      "Iter2560\n",
      "Minibatch Loss=1.503160, Training Accuracy=0.39844\n",
      "Iter3840\n",
      "Minibatch Loss=1.436678, Training Accuracy=0.49219\n",
      "Iter5120\n",
      "Minibatch Loss=0.994916, Training Accuracy=0.65625\n",
      "Iter6400\n",
      "Minibatch Loss=0.754417, Training Accuracy=0.78906\n",
      "Iter7680\n",
      "Minibatch Loss=0.775053, Training Accuracy=0.78125\n",
      "Iter8960\n",
      "Minibatch Loss=0.583514, Training Accuracy=0.80469\n",
      "Iter10240\n",
      "Minibatch Loss=0.519098, Training Accuracy=0.82031\n",
      "Iter11520\n",
      "Minibatch Loss=0.559341, Training Accuracy=0.82031\n",
      "Iter12800\n",
      "Minibatch Loss=0.511626, Training Accuracy=0.79688\n",
      "Iter14080\n",
      "Minibatch Loss=0.336857, Training Accuracy=0.89844\n",
      "Iter15360\n",
      "Minibatch Loss=0.435611, Training Accuracy=0.88281\n",
      "Iter16640\n",
      "Minibatch Loss=0.423179, Training Accuracy=0.89062\n",
      "Iter17920\n",
      "Minibatch Loss=0.327444, Training Accuracy=0.90625\n",
      "Iter19200\n",
      "Minibatch Loss=0.269810, Training Accuracy=0.91406\n",
      "Iter20480\n",
      "Minibatch Loss=0.282459, Training Accuracy=0.87500\n",
      "Iter21760\n",
      "Minibatch Loss=0.249996, Training Accuracy=0.92188\n",
      "Iter23040\n",
      "Minibatch Loss=0.332345, Training Accuracy=0.88281\n",
      "Iter24320\n",
      "Minibatch Loss=0.236442, Training Accuracy=0.92188\n",
      "Iter25600\n",
      "Minibatch Loss=0.190168, Training Accuracy=0.96094\n",
      "Iter26880\n",
      "Minibatch Loss=0.177907, Training Accuracy=0.93750\n",
      "Iter28160\n",
      "Minibatch Loss=0.249763, Training Accuracy=0.89844\n",
      "Iter29440\n",
      "Minibatch Loss=0.176990, Training Accuracy=0.95312\n",
      "Iter30720\n",
      "Minibatch Loss=0.275230, Training Accuracy=0.90625\n",
      "Iter32000\n",
      "Minibatch Loss=0.172570, Training Accuracy=0.94531\n",
      "Iter33280\n",
      "Minibatch Loss=0.240853, Training Accuracy=0.90625\n",
      "Iter34560\n",
      "Minibatch Loss=0.277626, Training Accuracy=0.92969\n",
      "Iter35840\n",
      "Minibatch Loss=0.115368, Training Accuracy=0.95312\n",
      "Iter37120\n",
      "Minibatch Loss=0.334413, Training Accuracy=0.92969\n",
      "Iter38400\n",
      "Minibatch Loss=0.207837, Training Accuracy=0.93750\n",
      "Iter39680\n",
      "Minibatch Loss=0.142769, Training Accuracy=0.96094\n",
      "Iter40960\n",
      "Minibatch Loss=0.210287, Training Accuracy=0.91406\n",
      "Iter42240\n",
      "Minibatch Loss=0.196102, Training Accuracy=0.94531\n",
      "Iter43520\n",
      "Minibatch Loss=0.174191, Training Accuracy=0.95312\n",
      "Iter44800\n",
      "Minibatch Loss=0.172105, Training Accuracy=0.93750\n",
      "Iter46080\n",
      "Minibatch Loss=0.190887, Training Accuracy=0.93750\n",
      "Iter47360\n",
      "Minibatch Loss=0.074227, Training Accuracy=0.97656\n",
      "Iter48640\n",
      "Minibatch Loss=0.075487, Training Accuracy=0.97656\n",
      "Iter49920\n",
      "Minibatch Loss=0.182841, Training Accuracy=0.96094\n",
      "Iter51200\n",
      "Minibatch Loss=0.117764, Training Accuracy=0.96094\n",
      "Iter52480\n",
      "Minibatch Loss=0.205150, Training Accuracy=0.91406\n",
      "Iter53760\n",
      "Minibatch Loss=0.077372, Training Accuracy=0.97656\n",
      "Iter55040\n",
      "Minibatch Loss=0.117225, Training Accuracy=0.96875\n",
      "Iter56320\n",
      "Minibatch Loss=0.121377, Training Accuracy=0.98438\n",
      "Iter57600\n",
      "Minibatch Loss=0.195023, Training Accuracy=0.96094\n",
      "Iter58880\n",
      "Minibatch Loss=0.083396, Training Accuracy=0.97656\n",
      "Iter60160\n",
      "Minibatch Loss=0.208886, Training Accuracy=0.92188\n",
      "Iter61440\n",
      "Minibatch Loss=0.119412, Training Accuracy=0.95312\n",
      "Iter62720\n",
      "Minibatch Loss=0.105853, Training Accuracy=0.96875\n",
      "Iter64000\n",
      "Minibatch Loss=0.227048, Training Accuracy=0.94531\n",
      "Iter65280\n",
      "Minibatch Loss=0.126184, Training Accuracy=0.96875\n",
      "Iter66560\n",
      "Minibatch Loss=0.171776, Training Accuracy=0.94531\n",
      "Iter67840\n",
      "Minibatch Loss=0.061200, Training Accuracy=0.98438\n",
      "Iter69120\n",
      "Minibatch Loss=0.077394, Training Accuracy=0.96875\n",
      "Iter70400\n",
      "Minibatch Loss=0.109481, Training Accuracy=0.96094\n",
      "Iter71680\n",
      "Minibatch Loss=0.168135, Training Accuracy=0.96094\n",
      "Iter72960\n",
      "Minibatch Loss=0.126411, Training Accuracy=0.94531\n",
      "Iter74240\n",
      "Minibatch Loss=0.120576, Training Accuracy=0.96875\n",
      "Iter75520\n",
      "Minibatch Loss=0.165406, Training Accuracy=0.95312\n",
      "Iter76800\n",
      "Minibatch Loss=0.090900, Training Accuracy=0.96094\n",
      "Iter78080\n",
      "Minibatch Loss=0.090726, Training Accuracy=0.96875\n",
      "Iter79360\n",
      "Minibatch Loss=0.109495, Training Accuracy=0.96875\n",
      "Iter80640\n",
      "Minibatch Loss=0.110555, Training Accuracy=0.96875\n",
      "Iter81920\n",
      "Minibatch Loss=0.164620, Training Accuracy=0.96094\n",
      "Iter83200\n",
      "Minibatch Loss=0.073007, Training Accuracy=0.97656\n",
      "Iter84480\n",
      "Minibatch Loss=0.090647, Training Accuracy=0.97656\n",
      "Iter85760\n",
      "Minibatch Loss=0.118515, Training Accuracy=0.97656\n",
      "Iter87040\n",
      "Minibatch Loss=0.087962, Training Accuracy=0.96875\n",
      "Iter88320\n",
      "Minibatch Loss=0.135431, Training Accuracy=0.96094\n",
      "Iter89600\n",
      "Minibatch Loss=0.087417, Training Accuracy=0.97656\n",
      "Iter90880\n",
      "Minibatch Loss=0.077991, Training Accuracy=0.96875\n",
      "Iter92160\n",
      "Minibatch Loss=0.051879, Training Accuracy=0.99219\n",
      "Iter93440\n",
      "Minibatch Loss=0.120386, Training Accuracy=0.97656\n",
      "Iter94720\n",
      "Minibatch Loss=0.133558, Training Accuracy=0.97656\n",
      "Iter96000\n",
      "Minibatch Loss=0.106538, Training Accuracy=0.95312\n",
      "Iter97280\n",
      "Minibatch Loss=0.112663, Training Accuracy=0.97656\n",
      "Iter98560\n",
      "Minibatch Loss=0.072754, Training Accuracy=0.98438\n",
      "Iter99840\n",
      "Minibatch Loss=0.062979, Training Accuracy=0.97656\n",
      "Optimization Finished\n",
      "Testing Accuracy: 0.9921875\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step=1\n",
    "    while step*batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_inputs))\n",
    "        \n",
    "        sess.run(optimizer, feed_dict = {x:batch_x, y:batch_y})\n",
    "        if step%display_step == 0:\n",
    "            acc =sess.run(accuracy, feed_dict={x:batch_x, y:batch_y})\n",
    "            loss = sess.run(cost, feed_dict={x:batch_x, y:batch_y})\n",
    "            print('Iter'+str(step*batch_size)+'\\nMinibatch Loss='\n",
    "                 + '{:.6f}'.format(loss)+', Training Accuracy='\n",
    "                 + '{:.5f}'.format(acc))\n",
    "        step += 1\n",
    "    print('Optimization Finished')\n",
    "    \n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1,n_steps, n_inputs))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print('Testing Accuracy:', sess.run(accuracy, feed_dict={x:test_data, y:test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
